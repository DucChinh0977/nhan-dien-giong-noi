# app.py
import pyaudio
import numpy as np
import torch
import torchaudio
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor
import wave
import os
import time

# Cáº¥u hÃ¬nh Silero VAD
torch.set_num_threads(1)
vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)
(get_speech_timestamps, _, read_audio, *_) = utils

# Cáº¥u hÃ¬nh ghi Ã¢m
CHUNK = 512
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
SILENCE_DURATION = 1.0
SPEECH_THRESHOLD = 0.5
MIN_SPEECH_DURATION = 0.3

# Cáº¥u hÃ¬nh mÃ´ hÃ¬nh Speech-to-Text
STT_MODEL_PATH = "wav2vec2_vietnamese_speech_to_text_model"
stt_processor = Wav2Vec2Processor.from_pretrained(STT_MODEL_PATH)
stt_model = Wav2Vec2ForCTC.from_pretrained(STT_MODEL_PATH)
stt_model.eval()

# Cáº¥u hÃ¬nh mÃ´ hÃ¬nh Speaker Identification
SPEAKER_MODEL_PATH = "wav2vec2_speaker_id_model"
speaker_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(SPEAKER_MODEL_PATH)
speaker_model = Wav2Vec2ForSequenceClassification.from_pretrained(SPEAKER_MODEL_PATH)
speaker_model.eval()

# Ãnh xáº¡ nhÃ£n ngÆ°á»i nÃ³i
label_map = {0: "Chinh", 1: "Viet", 2: "VietLoi"}

# Khá»Ÿi táº¡o PyAudio
p = pyaudio.PyAudio()
stream = p.open(format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK)


# HÃ m nháº­n dáº¡ng ngÆ°á»i nÃ³i
def identify_speaker(audio_path):
    waveform, sample_rate = torchaudio.load(audio_path)
    if sample_rate != 16000:
        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)

    inputs = speaker_feature_extractor(
        waveform.squeeze(0),
        sampling_rate=16000,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=80000
    )
    with torch.no_grad():
        logits = speaker_model(inputs.input_values).logits
    predicted_id = torch.argmax(logits, dim=-1).item()
    speaker = label_map[predicted_id]
    return speaker


# HÃ m chuyá»ƒn Ä‘á»•i Ã¢m thanh thÃ nh vÄƒn báº£n
def speech_to_text(audio_path):
    waveform, sample_rate = torchaudio.load(audio_path)
    if sample_rate != 16000:
        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)

    inputs = stt_processor(waveform.squeeze(0), sampling_rate=16000, return_tensors="pt", padding=True)
    with torch.no_grad():
        logits = stt_model(inputs.input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = stt_processor.batch_decode(predicted_ids)[0]
    return transcription


# HÃ m lÆ°u Ä‘oáº¡n Ã¢m thanh vÃ o file .wav
def save_audio(frames, filename):
    wf = wave.open(filename, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()


# HÃ m phÃ¡t hiá»‡n giá»ng nÃ³i báº±ng Silero VAD
def detect_speech(audio_chunk):
    audio_int16 = np.frombuffer(audio_chunk, dtype=np.int16)
    audio_float32 = audio_int16.astype(np.float32) / 32768.0
    audio_tensor = torch.tensor(audio_float32)
    speech_prob = vad_model(audio_tensor, RATE).item()
    return speech_prob > SPEECH_THRESHOLD


# HÃ m lÆ°u káº¿t quáº£ vÃ o file
def save_result(speaker, transcription):
    with open("results.txt", "a", encoding="utf-8") as f:
        f.write(f"{speaker}: {transcription}\n")


# HÃ m chÃ­nh Ä‘á»ƒ ghi Ã¢m vÃ  xá»­ lÃ½ theo thá»i gian thá»±c
def main():
    print("Báº¯t Ä‘áº§u chÆ°Æ¡ng trÃ¬nh Speech-to-Text vá»›i VAD vÃ  Speaker Identification...")
    print("Äang láº¯ng nghe... NÃ³i Ä‘á»ƒ ghi Ã¢m, im láº·ng trong 1 giÃ¢y Ä‘á»ƒ dá»«ng.")

    audio_buffer = []
    is_speaking = False
    silence_start = None
    recording_count = 0

    # XÃ³a file results.txt náº¿u Ä‘Ã£ tá»“n táº¡i
    if os.path.exists("results.txt"):
        os.remove("results.txt")

    while True:
        try:
            data = stream.read(CHUNK, exception_on_overflow=False)

            if detect_speech(data):
                if not is_speaking:
                    print("ðŸ—£ï¸ PhÃ¡t hiá»‡n giá»ng nÃ³i, báº¯t Ä‘áº§u ghi Ã¢m...")
                    is_speaking = True
                    audio_buffer = []
                audio_buffer.append(data)
                silence_start = None
            else:
                if is_speaking:
                    if silence_start is None:
                        silence_start = time.time()
                    elif time.time() - silence_start >= SILENCE_DURATION:
                        print("ðŸ¤« Im láº·ng, dá»«ng ghi Ã¢m...")
                        is_speaking = False

                        recording_count += 1
                        audio_path = f"recording_{recording_count}.wav"
                        save_audio(audio_buffer, audio_path)

                        waveform, _ = torchaudio.load(audio_path)
                        duration = waveform.shape[1] / RATE
                        if duration < MIN_SPEECH_DURATION:
                            print(f"Äoáº¡n Ã¢m thanh quÃ¡ ngáº¯n ({duration:.2f} giÃ¢y), bá» qua...")
                            os.remove(audio_path)
                            continue

                        print("Nháº­n dáº¡ng ngÆ°á»i nÃ³i...")
                        speaker = identify_speaker(audio_path)

                        print("Chuyá»ƒn Ä‘á»•i Ã¢m thanh thÃ nh vÄƒn báº£n...")
                        transcription = speech_to_text(audio_path)
                        print(f"{speaker}: {transcription}")

                        # LÆ°u káº¿t quáº£ vÃ o file
                        save_result(speaker, transcription)

                        os.remove(audio_path)

            if not is_speaking:
                audio_buffer = []

        except KeyboardInterrupt:
            print("\nDá»«ng chÆ°Æ¡ng trÃ¬nh...")
            break

    stream.stop_stream()
    stream.close()
    p.terminate()


if __name__ == "__main__":
    main()